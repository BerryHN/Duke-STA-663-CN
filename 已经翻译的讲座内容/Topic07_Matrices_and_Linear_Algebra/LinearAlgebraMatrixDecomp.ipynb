{
 "metadata": {
  "name": "",
  "signature": "sha256:1e6433efdd45c31405d1cc52316807536f8ed3d1ab5de69f7138d4adfcd9d2e8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Reference**\n",
      "\n",
      "[SciPy's official tutorial on Linear algebra](http://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "大型线性系统"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the age of Big Data.  Every second of every day, data is being recorded in countless systems over the world.  Our shopping habits, book and movie preferences, key words typed into our email messages, medical records, NSA recordings of our telephone calls, genomic data - and none of it is any use without analysis."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enormous data sets carry with them enormous challenges in data processing.  Solving a system of $10$ equations in $10$ unknowns is easy, and one need not be terribly careful about methodolgy.  But as the size of the system grows, algorithmic complexity and efficiency become critical."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example: Netflix Competition (circa 2006-2009)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a more complete description:\n",
      "    \n",
      "http://en.wikipedia.org/wiki/Netflix_Prize\n",
      "        \n",
      "The whole technical story\n",
      "\n",
      "http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In 2006, Netflix opened a competition where it provided ratings of over $400,000$ for $18,000$ movies.  The goal was to make predict a user's rating of a movie, based on previous ratings *and* ratings of 'similar' users.  The task amounted to analysis of a $400,000\\times 18,000$ matrix!  The wikipedia link above describes the contest and the second link is a very detailed description of the method (which took into account important characteristics such as how tastes may change over time).  Part of the analysis is related to matrix decomposition - we won't go into the details of the winning algorithm, but we will spend some time on basic matrix decompositions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Matrix Decompositions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Matrix decompositions are an important step in solving linear systems in a computationally efficient manner. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "LU Decomposition and Gaussian Elimination"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LU stands for 'Lower Upper', and so an LU decomposition of a matrix $A$ is a decomposition so that \n",
      "$$A= LU$$\n",
      "where $L$ is lower triangular and $U$ is upper triangular.\n",
      "\n",
      "Now, LU decomposition is essentially gaussian elimination, but we work only with the matrix $A$ (as opposed to the augmented matrix). \n",
      "\n",
      "Let's review how gaussian elimination (ge) works.  We will deal with a $3\\times 3$ system of equations for conciseness, but everything here generalizes to the $n\\times n$ case. Consider the following equation:\n",
      "\n",
      "$$\\left(\\begin{matrix}a_{11}&a_{12} & a_{13}\\\\a_{21}&a_{22}&a_{23}\\\\a_{31}&a_{32}&a_{33}\\end{matrix}\\right)\\left(\\begin{matrix}x_1\\\\x_2\\\\x_3\\end{matrix}\\right) = \\left(\\begin{matrix}b_1\\\\b_2\\\\b_3\\end{matrix}\\right)$$\n",
      "\n",
      "For simplicity, let us assume that the leftmost matrix $A$ is non-singular.  To solve the system using ge, we start with the 'augmented matrix':\n",
      "\n",
      "$$\\left(\\begin{array}{ccc|c}a_{11}&a_{12} & a_{13}& b_1 \\\\a_{21}&a_{22}&a_{23}&b_2\\\\a_{31}&a_{32}&a_{33}&b_3\\end{array}\\right)$$\n",
      "\n",
      "We begin at the first entry, $a_{11}$.  If $a_{11} \\neq 0$, then we divide the first row by $a_{11}$ and then subtract the appropriate multiple of the first row from each of the other rows, zeroing out the first entry of all rows. (If $a_{11}$ is zero, we need to permute rows.  We will not go into detail of that here.)  The result is as follows:\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\left(\\begin{array}{ccc|c}\n",
      "1 & \\frac{a_{12}}{a_{11}} & \\frac{a_{13}}{a_{11}} & \\frac{b_1}{a_{11}} \\\\\n",
      "0 & a_{22} - a_{21}\\frac{a_{12}}{a_{11}} & a_{23} - a_{21}\\frac{a_{13}}{a_{11}}  & b_2 - a_{21}\\frac{b_1}{a_{11}}\\\\\n",
      "0&a_{32}-a_{31}\\frac{a_{12}}{a_{11}} & a_{33} - a_{31}\\frac{a_{13}}{a_{11}}  &b_3- a_{31}\\frac{b_1}{a_{11}}\\end{array}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We repeat the procedure for the second row, first dividing by the leading entry, then subtracting the appropriate multiple of the resulting row from each of the third and first rows, so that the second entry in row 1 and in row 3 are zero.  We *could* continue until the matrix on the left is the identity. In that case, we can then just 'read off' the solution: i.e., the vector $x$ is the resulting column vector on the right.  Usually, it is more efficient to stop at *reduced row eschelon* form (upper triangular, with ones on the diagonal), and then use *back substitution* to obtain the final answer.\n",
      "\n",
      "Note that in some cases, it is necessary to permute rows to obtain reduced row eschelon form.  This is called *partial pivoting*.  If we also manipulate columns, that is called *full pivoting*.\n",
      "\n",
      "It should be mentioned that we may obtain the inverse of a matrix using ge, by reducing the matrix $A$ to the identity, with the identity matrix as the augmented portion.  \n",
      "\n",
      "Now, this is all fine when we are solving a system one time, for one outcome $b$.  Many applications involve solutions to multiple problems, where the left-hand-side of our matrix equation does not change, but there are many outcome vectors $b$.  In this case, it is more efficient to *decompose* $A$.\n",
      "\n",
      "First, we start just as in ge, but we 'keep track' of the various multiples required to eliminate entries.  For example, consider the matrix\n",
      "\n",
      "$$A = \\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
      "                           2& 1& 3\\\\\n",
      "                           4&1&2\n",
      "                           \\end{matrix}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to multiply row $1$ by $2$ and subtract from row $2$ to eliminate the first entry in row $2$, and then multiply row $1$ by $4$ and subtract from row $3$. Instead of entering zeroes into the first entries of rows $2$ and $3$, we record the multiples required for their elimination, as so:\n",
      "\n",
      "$$\\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
      "                           (2)& -5 & -5\\\\\n",
      "                           (4)&-11&-14\n",
      "                           \\end{matrix}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "And then we eliminate the second entry in the third row:\n",
      "\n",
      "\n",
      "$$\\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
      "                           (2)& -5 & -5\\\\\n",
      "                           (4)&(\\frac{-11}{5})&-3\n",
      "                           \\end{matrix}\\right)$$\n",
      "                           \n",
      "And now we have the decomposition:\n",
      "$$L= \\left(\\begin{matrix} 1 & 0 & 0 \\\\\n",
      "                           2& 1 & 0\\\\\n",
      "                           4&\\frac{-11}5&1\n",
      "                           \\end{matrix}\\right)\n",
      "                          U = \\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
      "                           0& -5 & -5\\\\\n",
      "                           0&0&-3\n",
      "                           \\end{matrix}\\right)$$\n",
      "                                                  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can solve the system by solving two back-substitution problems:\n",
      "    \n",
      "$$Ly = b$$    and\n",
      "$$Ux=y$$\n",
      "\n",
      "\n",
      "These are both $O(n^2)$, so it is more efficient to decompose when there are multiple outcomes to solve for."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let do this with numpy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.linalg as la\n",
      "np.set_printoptions(suppress=True) \n",
      "\n",
      "A = np.array([[1,3,4],[2,1,3],[4,1,2]])\n",
      "\n",
      "print(A)\n",
      "\n",
      "P, L, U = la.lu(A)\n",
      "print(np.dot(P.T, A))\n",
      "print\n",
      "print(np.dot(L, U))\n",
      "print(P)\n",
      "print(L)\n",
      "print(U)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 3 4]\n",
        " [2 1 3]\n",
        " [4 1 2]]\n",
        "[[ 4.  1.  2.]\n",
        " [ 1.  3.  4.]\n",
        " [ 2.  1.  3.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[[ 4.  1.  2.]\n",
        " [ 1.  3.  4.]\n",
        " [ 2.  1.  3.]]\n",
        "[[ 0.  1.  0.]\n",
        " [ 0.  0.  1.]\n",
        " [ 1.  0.  0.]]\n",
        "[[ 1.      0.      0.    ]\n",
        " [ 0.25    1.      0.    ]\n",
        " [ 0.5     0.1818  1.    ]]\n",
        "[[ 4.      1.      2.    ]\n",
        " [ 0.      2.75    3.5   ]\n",
        " [ 0.      0.      1.3636]]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the numpy decomposition uses *partial pivoting* (matrix rows are permuted to use the largest pivot).  This is because small pivots can lead to numerical instability.  Another reason why one should use library functions whenever possible!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cholesky Decomposition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that a square matrix $A$ is positive definite if\n",
      "\n",
      "$$u^TA u > 0$$\n",
      "\n",
      "for any non-zero n-dimensional vector $u$,\n",
      "\n",
      "and a symmetric, positive-definite matrix $A$ is a positive-definite matrix such that\n",
      "\n",
      "$$A = A^T$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $A$ be a symmetric, positive-definite matrix.  There is a unique decomposition such that\n",
      "\n",
      "$$A = L L^T$$\n",
      "\n",
      "where $L$ is lower-triangular with positive diagonal elements and $L^T$ is its transpose.  This decomposition is known as the Cholesky decompostion, and $L$ may be interpreted as the 'square root' of the matrix $A$.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Algorithm:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $A$ be an $n\\times n$ matrix.  We find the matri $L$ using the following iterative procedure:\n",
      "\n",
      "\n",
      "$$A = \\left(\\begin{matrix}a_{11}&A_{12}\\\\A_{12}&A_{22}\\end{matrix}\\right) =\n",
      "\\left(\\begin{matrix}\\ell_{11}&0\\\\\n",
      "L_{12}&L_{22}\\end{matrix}\\right)\n",
      "\\left(\\begin{matrix}\\ell_{11}&L_{12}\\\\0&L_{22}\\end{matrix}\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.) Let $\\ell_{11} = \\sqrt{a_{11}}$\n",
      "\n",
      "2.) $L_{12} = \\frac{1}{\\ell_{11}}A_{12}$\n",
      "\n",
      "3.) Solve $A_{22} - L_{12}L_{12}^T = L_{22}L_{22}^T$ for $L_{22}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$A = \\left(\\begin{matrix}1&3&5\\\\3&13&23\\\\5&23&42\\end{matrix}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\ell_{11} = \\sqrt{a_{11}} = 1$$\n",
      "\n",
      "$$L_{12} = \\frac{1}{\\ell_{11}} A_{12} = A_{12}$$\n",
      "\n",
      "$\\begin{eqnarray*}\n",
      "A_22 - L_{12}L_{12}^T &=& \\left(\\begin{matrix}13&23\\\\23&42\\end{matrix}\\right) - \\left(\\begin{matrix}9&15\\\\15&25\\end{matrix}\\right)\\\\\n",
      "&=& \\left(\\begin{matrix}4&8\\\\8&17\\end{matrix}\\right)\\\\\n",
      "&=& \\left(\\begin{matrix}2&0\\\\4&\\ell_{33}\\end{matrix}\\right) \\left(\\begin{matrix}2&4\\\\0&\\ell_{33}\\end{matrix}\\right)\\\\\n",
      "&=& \\left(\\begin{matrix}4&8\\\\8&16+\\ell_{33}^2\\end{matrix}\\right)\n",
      "\\end{eqnarray*}$\n",
      "\n",
      "And so we conclude that $\\ell_{33}=1$.\n",
      "\n",
      "\n",
      "This yields the decomposition:\n",
      "\n",
      "\n",
      "$$\\left(\\begin{matrix}1&3&5\\\\3&13&23\\\\5&23&42\\end{matrix}\\right) = \n",
      "\\left(\\begin{matrix}1&0&0\\\\3&2&0\\\\5&4&1\\end{matrix}\\right)\\left(\\begin{matrix}1&3&5\\\\0&2&4\\\\0&0&1\\end{matrix}\\right)$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, with numpy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[1,3,5],[3,13,23],[5,23,42]])\n",
      "L = la.cholesky(A)\n",
      "print(np.dot(L.T, L))\n",
      "\n",
      "print(L)\n",
      "print(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  1.   3.   5.]\n",
        " [  3.  13.  23.]\n",
        " [  5.  23.  42.]]\n",
        "[[ 1.  3.  5.]\n",
        " [ 0.  2.  4.]\n",
        " [ 0.  0.  1.]]\n",
        "[[ 1  3  5]\n",
        " [ 3 13 23]\n",
        " [ 5 23 42]]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cholesky decomposition is about twice as fast as LU decomposition (though both scale as $n^3$)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Matrix Decompositions for PCA and Least Squares"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "特征值分解"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "特征向量和特征值"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "首先回忆，矩阵 $A$ 的*特征向量*是一个非零向量 $v$，对于一些标量 $\\lambda$，满足：\n",
      "\n",
      "$$Av = \\lambda v$$\n",
      "\n",
      "值 $\\lambda$ 就叫做 $A$ 的*特征值*。\n",
      "\n",
      "如果 $n\\times n$ 的矩阵 $A$ 拥有 $n$ 个线性无关的特征向量，那么 $A$ 就可以用这种方式分解：\n",
      "\n",
      "$$A = B\\Lambda B^{-1}$$\n",
      "\n",
      "其中 $\\Lambda$ 是一个对角矩阵，对角线上的元素是 $A$ 的特征值，$B$ 的列向量是 $A$ 的对应特征向量。\n",
      "\n",
      "定理：\n",
      "\n",
      "* $n\\times n$ 的矩阵是可对角化的 $\\iff$ 它有 $n$ 个线性无关的特征向量。\n",
      "* 对称的正定阵只拥有正的特征值，并且它的特征值分解\n",
      "$$A=B\\Lambda B^{-1}$$\n",
      "\n",
      "通过正交变换 $B$ 来实现。（也就是，它的特征向量使标准正交集）\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "计算特征值"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "从定义中很容易看出，如果 $v$ 是 $n\\times n$ 矩阵 $A$ 的特征向量，拥有特征值 $\\lambda$，那么\n",
      "\n",
      "$$(A - \\lambda I)v = \\bf{0}$$\n",
      "\n",
      "其中 $I$ 是 $n$ 维单位矩阵，$\\bf{0}$ 是 n 维零向量。因此，$A$ 的特征值满足：\n",
      "\n",
      "$$\\det\\left(A-\\lambda I\\right)=0$$\n",
      "\n",
      "上式的左边是 $\\lambda$ 的多项式，叫做 $A$ 的*特征多项式*。所以，为了找到 $A$ 的特征值，我们需要找到特征多项式的根。\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "但是，从计算上来说，计算特征多项式以及求解根，是极其复杂的。因此，在实践中，会使用数值方法 -- 来找到特征值以及相应的特征向量。我们不会陷入用于计算特征值的算法细节，这里是个 NumPy 的示例："
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[0,1,1],[2,1,0],[3,4,5]])\n",
      "\n",
      "u, V = la.eig(A)\n",
      "print(np.dot(V,np.dot(np.diag(u), la.inv(V))))\n",
      "print(u)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.+0.j  1.+0.j  1.+0.j]\n",
        " [ 2.+0.j  1.+0.j  0.+0.j]\n",
        " [ 3.+0.j  4.+0.j  5.+0.j]]\n",
        "[ 5.8541+0.j -0.8541+0.j  1.0000+0.j]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**注意：** 许多矩阵*无法*对角化，许多矩阵拥有*复数*特征值（以及所有元素都是实数）。"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[0,1],[-1,0]])\n",
      "print(A)\n",
      "\n",
      "u, V = la.eig(A)\n",
      "print(np.dot(V,np.dot(np.diag(u), la.inv(V))))\n",
      "print(u)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0  1]\n",
        " [-1  0]]\n",
        "[[ 0.+0.j  1.+0.j]\n",
        " [-1.+0.j  0.+0.j]]\n",
        "[ 0.+1.j  0.-1.j]\n",
        "[ 0.+1.j  0.-1.j]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# If you know the eigenvalues must be reeal \n",
      "# because A is a positive definite (e.g. covariance) matrix \n",
      "# use real_if_close\n",
      "\n",
      "A = np.array([[0,1,1],[2,1,0],[3,4,5]])\n",
      "u, V = la.eig(A)\n",
      "print(u)\n",
      "print np.real_if_close(u)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 5.8541+0.j -0.8541+0.j  1.0000+0.j]\n",
        "[ 5.8541 -0.8541  1.    ]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "奇异值"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "对于任何 $m\\times n$ 的矩阵 $A$，我们将它的*奇异值*定义为 $A^TA$ 特征值的平方根。这是一个良好的定义，因为 $A^TA$ 永远是对称，并且正定的，所以它的特征值都是正实数。奇异值是矩阵的非常重要的属性。几何上，矩阵 $A$ 将 $\\mathbb{R}^n$ 中的单位球映射为椭圆。 奇异值是半轴的长度。\n",
      "\n",
      "奇异值也提供了矩阵*稳定性*的度量。我们会在这篇文章末尾再次看到它。"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "QR decompositon"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with the previous decompositions, $QR$ decomposition is a method to write a matrix $A$ as the product of two matrices of simpler form.  In this case, we want:\n",
      "\n",
      "$$ A= QR$$\n",
      "where $Q$ is an $m\\times n$ matrix with $Q Q^T = I$ (i.e. $Q$ is *orthogonal*) and $R$ is an $n\\times n$ upper-triangular matrix.\n",
      "\n",
      "This is really just the matrix form of the Gram-Schmidt orthogonalization of the columns of $A$.  The G-S algorithm itself is unstable, so various other methods have been developed to compute the QR decomposition.  We won't cover those in detail as they are a bit beyond our scope."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first $k$ columns of $Q$ are an orthonormal basis for the column space of the first $k$ columns of $A$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Iterative QR decomposition is often used in the computation of eigenvalues."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Singular Value Decomposition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another important matrix decomposition is singular value decomposition or SVD.   For any $m\\times n$ matrix $A$, we may write:\n",
      "\n",
      "$$A= UDV$$\n",
      "\n",
      "where $U$ is a unitary (orthogonal in the real case) $m\\times m$ matrix, $D$ is a rectangular, diagonal $m\\times n$ matrix with diagonal entries $d_1,...,d_m$ all non-negative. $V$ is a unitary (orthogonal) $n\\times n$ matrix. SVD is used in principle component analysis and in the computation of the Moore-Penrose pseudo-inverse."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stabilty and Condition Number"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is important that numerical algorithms be *stable* and *efficient*.  Efficiency is a property of an algorithm, but stability can be a property of the system itself."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\left(\\begin{matrix}8&6&4&1\\\\1&4&5&1\\\\8&4&1&1\\\\1&4&3&6\\end{matrix}\\right)x = \\left(\\begin{matrix}19\\\\11\\\\14\\\\14\\end{matrix}\\right)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[8,6,4,1],[1,4,5,1],[8,4,1,1],[1,4,3,6]])\n",
      "b = np.array([19,11,14,14])\n",
      "la.solve(A,b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "array([ 1.,  1.,  1.,  1.])"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b = np.array([19.01,11.05,14.07,14.05])\n",
      "la.solve(A,b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "array([-2.34 ,  9.745, -4.85 , -1.34 ])"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the *tiny* perturbations in the outcome vector $b$ cause *large* differences in the solution!  When this happens, we say that the matrix $A$ *ill-conditioned*.  This happens when a matrix is 'close' to being singular (i.e. non-invertible)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Condition Number"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "  \n",
      "\n",
      "A measure of this type of behavior is called the *condition number*.  It is defined as:\n",
      "\n",
      "$$ cond(A) = ||A||\\cdot ||A^{-1}|| $$\n",
      "\n",
      "In general, it is difficult to compute.\n",
      "\n",
      "Fact: \n",
      "\n",
      "$$cond(A) = \\frac{\\lambda_1}{\\lambda_n}$$\n",
      "\n",
      "where $\\lambda_1$ is the maximum singular value of $A$ and $\\lambda_n$ is the smallest.  The higher the condition number, the more unstable the system.  In general if there is a large discrepancy between minimal and maximal singular values, the condition number is large."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "U, s, V = np.linalg.svd(A)\n",
      "print(s)\n",
      "print(max(s)/min(s))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 15.5457   6.9002   3.8363   0.0049]\n",
        "3198.6725812\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Preconditioning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can sometimes improve on this behavior by 'pre-conditioning'.  Instead of solving\n",
      "$$Ax=b$$\n",
      "we solve\n",
      "$$D^{-1}Ax=D^{-1}b$$\n",
      "where $D^{-1}A$ has a lower condition number than $A$ itself.   \n",
      "\n",
      "Preconditioning is a *very* involved topic, quite out of the range of this course.  It is mentioned here only to make you aware that such a thing exists, should you ever run into an ill-conditioned problem!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=red>Exercises</font>\n",
      "----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1**. Compute the LU decomposition of the following matrix by hand and using numpy\n",
      "\n",
      "$$\\left(\\begin{matrix}1&2&3\\\\2&-4&6\\\\3&-9&-3\\end{matrix}\\right)$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2**. Compute the Cholesky decomposition of the following matrix by hand and using numpy\n",
      "\n",
      "$$\\left(\\begin{matrix}1&2&3\\\\2&-4&6\\\\3&6&-3\\end{matrix}\\right)$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3**. Write a function in Python to solve a system\n",
      "\n",
      "$$Ax = b$$\n",
      "\n",
      "using SVD decomposition.  Your function should take $A$ and $b$ as input and return $x$.\n",
      "\n",
      "Your function should include the following:\n",
      "\n",
      "* First, check that $A$ is invertible - return error message if it is not\n",
      "* Invert $A$ using SVD and solve\n",
      "* return $x$\n",
      "\n",
      "Test your function for correctness."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}