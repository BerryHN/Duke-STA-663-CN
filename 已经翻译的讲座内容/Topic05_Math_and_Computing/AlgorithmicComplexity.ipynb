{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%precision 4\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法复杂度（Algorithmic complexity）\n",
    "----\n",
    "\n",
    "数据结构，顾名思义就是用于存储数据的抽象结构。 咱们已经讲过了几个了，比如Python自带的列表list和字典dict等等。\n",
    "算法，则是用于处理各种数据结构的基本*配方recipes*。 当我们进行更多的计算密集型计算时，就需要更好地理解如何衡量数据结构和算法的性能，以便选择适当策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析和基准测试（Profling and benchmarking）\n",
    "\n",
    "我们可以根据执行的时间以及基本操作数来衡量性能。对运行时间的测量也称为分析或基准测试，在IPython Notebook里面可以方便地实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2 s per loop\n"
     ]
    }
   ],
   "source": [
    "# 使用 %timeit 来衡量函数调用需要的时间（to measure function calls）\n",
    "def f():\n",
    "    import time\n",
    "    time.sleep(2)\n",
    "\n",
    "%timeit -n1 f()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 衡量算法复杂度（Measuring algorithmic complexity）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, profiling doesn't tell us much about how the algorithm will perform on a different computer since it is partly determined by the hardware available.  To compare performance in a device-indpendent fashion, we use what is known as Big O notation (you may or may not have encountered this before in your Calculus courses). The Big O formalism characterizes functions in terms of their rates of growth. \n",
    "\n",
    "Suppsoe you wanted to search for an item in an unsorted list of length $n$. One way to do this would be to scan from the first position sequentially until you find it (or not). If the item is in the list, you will need to scan ($n/2$) items on average to find it. If it is not in the list, you will need to scan all $n$ items. In any case, the complexity of the search grows linearly with the lenght of the list $n$. We say that the algorithmic complexity of the search using a linear scan is $\\mathcal{O}(n)$.\n",
    "\n",
    "Strictly, we should say the *average* complexity is $\\mathcal{O}(n)$. We can also calculate worst case performance (when the item is not in the list), which is the same class $\\mathcal{O}(n)$ as average complexity for this searching example. Since worst case performance may require a perverse organizaiotn of the input (e.g. asking a sort function to sort an already sorted list), randomizaiton of inputs will sometimes suffice to convert it to the average case.\n",
    "\n",
    "A little more formally, we have a comparison function $g(n)$ and another function $f(n)$ that returns the number of \"elementary operations\" we need to perform in our algorithm given an input of size $n$. In the example, the elementary oepration is comparison of two items. In statisitcal algorithms, this is most commonly a floating point operation (FLOP), such as addition or multiplicaiton of two floats. Now if the ratio $|f(n)/g(n)|$ can be bounded by a finite number $M$ as $n$ grows to infinity, we say that $f(n)$ has complexity of order $g(n)$. For example, if $f(n) = 10n^2$ and $g(n) = n$, then there is no such number $M$ and $f(n)$ is **not** $\\mathcal{O}(n)$, but if $g(n) = n^2$, then $M = 10$ wil do and we say that $f(n)$ is $\\mathcal{O}(n^2)$. So our search function is $\\mathcal{O}(n)$. Formally, it is also $\\mathcal{O} (n^2)$ and so on, but we always choose the \"smallest\" function $g$. We also drop all terms ohter than the larget - so we don't say $\\mathcal{O}(n^3 + n^2 + n)$ but simply $\\mathcal{O}(n^3)$. \n",
    "\n",
    "Note that since the constant is not used in big O notation, two algorithms can have the same big O complexity and have very different performance! However, the O notation is very helpful for understanding the *scalability* of our algorithm. Below we show a comparison of an $\\mathcal{O}(n^2)$ algorithm (e.g. bubble sort) with an $\\mathcal{O}(n \\log{n})$ algorithm (e.g. merge sort). Regardless of the difference in constant factor, there is no competition as $n$ gets large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(n, k):\n",
    "    return k*n*n\n",
    "\n",
    "def f2(n, k):\n",
    "    return k*n*np.log(n)\n",
    "\n",
    "n = np.arange(0, 20001)\n",
    "\n",
    "plt.plot(n, f1(n, 1), c='blue')\n",
    "plt.plot(n, f2(n, 1000), c='red')\n",
    "plt.xlabel('Size of input (n)', fontsize=16)\n",
    "plt.ylabel('Number of operations', fontsize=16)\n",
    "plt.legend(['$\\mathcal{O}(n^2)$', '$\\mathcal{O}(n \\log n)$'], loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking of common Big O complexity classes\n",
    "\n",
    "- consstant = $\\mathcal{O}(1)$\n",
    "- logarithmic = $\\mathcal{O}(\\log n)$\n",
    "- linear = $\\mathcal{O}(n)$\n",
    "- n log n = $\\mathcal{O}(n \\log n)$\n",
    "- quadratic = $\\mathcal{O}(n^2)$\n",
    "- cubic = $\\mathcal{O}(n^3)$\n",
    "- polynomial = $\\mathcal{O}(n^k)$\n",
    "- exponential = $\\mathcal{O}(k^n)$\n",
    "- factorial =$\\mathcal{O}(n!)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://raw.githubusercontent.com/RehanSaeed/.NET-Big-O-Algorithm-Complexity-Cheat-Sheet/master/Cheat%20Sheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity of common operations on Python data structures\n",
    "\n",
    "See [here](https://wiki.python.org/moin/TimeComplexity) for the complexity of operations on standard Python data structures. Note for instance that searching a list is much more expensive than searching a dicitonary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching a list is O(n)\n",
    "\n",
    "alist = range(1000000)\n",
    "r = np.random.randint(100000)\n",
    "%timeit -n3 r in alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching a dictionary is O(1)\n",
    "\n",
    "adict = dict.fromkeys(alist)\n",
    "%timeit -n3 r in adict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space complexity\n",
    "----\n",
    "\n",
    "We can aslo use big O notnation in the same way to measure the space complexity of an algorithm. The basic idea is identical. The notion of space complexity becomes important when you data volume is  of the same magntude orlarger than the memory you have available. In that case, an algorihtm with high space complexity may end up having to swap memory constantly, and will perform far worse than its Big O for time complexity would suggest.\n",
    "\n",
    "Sometimes, you can trade space complexity for time complexity - caching and dynamic programming are obvious examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much space do I need?\n",
    "\n",
    "Just as you should have a good idea of how your algorithm will scale with increasing $n$, you should also be able to know how much memroy your data structures will require. For example, if you had an $n \\times p$ matrix of integers, an $n \\times p$ matrix of flaots, and an $n \\times p$ matrix of complex floats, how large can $n$ and $p$ be before you run out of RAM to store them? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "# Use %%time to measure evaluation cell the Unix way\n",
    "%time\n",
    "\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how much overhead Python objects have\n",
    "# A raw integer should be 64 bits or 8 bytes only\n",
    "\n",
    "print sys.getsizeof(1)\n",
    "print sys.getsizeof(1234567890123456789012345678901234567890)\n",
    "print sys.getsizeof(3.14)\n",
    "print sys.getsizeof(3j)\n",
    "print sys.getsizeof('a')\n",
    "print sys.getsizeof('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.ones((100,100), dtype='byte').nbytes\n",
    "print np.ones((100,100), dtype='i2').nbytes\n",
    "print np.ones((100,100), dtype='int').nbytes # default is 64 bits or 8 bytes\n",
    "print np.ones((100,100), dtype='f4').nbytes\n",
    "print np.ones((100,100), dtype='float').nbytes # default is 64 bits or 8 bytes\n",
    "print np.ones((100,100), dtype='complex').nbytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
